{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data files in Python\n",
    "\n",
    "In this notebook we'll cover the basics of working with data files in Python, from a space physics user perspective.\n",
    "\n",
    "Types of data file:\n",
    "- Plain text (and variants)\n",
    "- IDL save sets\n",
    "- NetCDF3\n",
    "- NetCDF4, HDF5, and Matlab save files\n",
    "- NASA CDF\n",
    "\n",
    "## Contents\n",
    "Here's what the notebook will cover. In the session at GEM we'll scroll through some of this pretty quickly, as the notebook is intended to be a resource.\n",
    "\n",
    "- Data, metadata, and data models\n",
    "- Setting up the Python environment\n",
    "- Getting files\n",
    "- Working with text\n",
    "- Legacy IDL save sets\n",
    "- Working with NetCDF3\n",
    "- HDF5, NetCDF4, and Matlab save files\n",
    "- NASA CDF\n",
    "\n",
    "## Prepping for the session\n",
    "Assuming we have a good internet conenction, you don't need to do any prep. *_BUT_*, we all know how conference wifi works out. So *if you want to grab the data in advance, just skip down to the \"Getting files\" section* and make sure you have all the data files. Nothing there is big, so it should all be fairly quick to retrieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, metadata, and data models\n",
    "\n",
    "We're all familiar with the concpet of data. Whatever our data source, we are almost certainly using numbers to represent a measurement (or simulated measurement). There are a few extra concpets that are helpful to be aware of for talking about data files, some (most? all?) of which will already be familiar to you.\n",
    "\n",
    "### Metadata\n",
    "\n",
    "Metadata is _data that describes data_. For example, I asked a coworker for a file containing electron flux data. They sent me a text file with 8 columns of numbers... What are the columns? What units do they have? This information is _metadata_.\n",
    "\n",
    "An easy way to provide basic metadata is labeling columns, the way you might in a spreadsheet. While that's better than nothing, there's so much more that can be done. What information might we want to store?\n",
    "- Units for each variable\n",
    "- Conversion factors (if not in SI units)\n",
    "- Long-form descriptions of variables\n",
    "We may also want metadata about the file, not just the variables:\n",
    "- Who is the instrument PI (or the originator of the data)\n",
    "- When was the file created?\n",
    "- What version number is this data?\n",
    "\n",
    "While some of this can be encoded in short-form headers, and in the file name itself, it can be difficult to know what's in the file without opening it and reading it. Also, while it is possible to represent data with more than two dimensions in text, this leads to design decisions that require custom tools for each specific data product, and metadata may or may not be included in the design.\n",
    "\n",
    "So a lot of what we'll be dealing with are binary files in _self-describing_ data formats.\n",
    "\n",
    "\n",
    "### Self-describing file formats and data models\n",
    "\n",
    "Ideally, the user won't need to know anything about how the data is stored in order to read it. The file should provide all that information in a discoverable way. Similarly, the _global_ metadata for the file should be available, as should the _per variable_ metadata.\n",
    "\n",
    "Most self-describing file formats are actually both a file format, and a software library that provides an interface to the data files. This means that the user doesn't need to know about how the data are stored, but the user still needs to know which interface to use (NASA CDF? HDF5?). The final thing to know is the data model - this is an abstraction of how the data are stored. Most modern self-describing formats use a data model that's analagous to a file system.\n",
    "\n",
    "<img src=\"images/datamodel.png\"  width=\"280\">\n",
    "\n",
    "In this schematic, we have a base folder (like the root directory on a linux-like file system). Attached to the base directory is global metadata. This is basically the README for the whole directory structure. What was the original filename of this self-describing file? What version of the processing code was the file made by?\n",
    "\n",
    "Then, inside the base directory we can also store data (analagous to files in a directory). Here we have a variable containing the common timebase, as well as two variables containing our data. Each of these variables have local metadata, such as units.\n",
    "\n",
    "The final entry at the base level is a variable group. This is like a subdirectory that can contain multiple variables all grouped together. It can carry metadata for the group. An example use-case might be for radiation belt electron phase space density. Calculating this requires a magnetic field model. So perhaps we want to group some of our variables by field model (PSD, McIlwain L, B<sub>0</sub> and B<sub>mirror</sub>).\n",
    "\n",
    "### Back to data formats\n",
    "\n",
    "This brings us back to the data formats we'll look at today.\n",
    "\n",
    "NASA CDF, NetCDF, and HDF are all self-describing formats. They all follow a data model similar to the one described above, with various restrictions. These formats are all pretty flexible, so we also need to know what metadata standard the files follow. NASA heliophysics mission all have to use the ISTP standard.\n",
    "- NASA CDF does not support nested groups\n",
    "  - There will only ever be one level in NASA CDF files. That is, you open the file, and all of your variables are right there. This can lead to logical groupings being done by varaible name, or by proliferation of files.\n",
    "  - CDF provides specific `epoch` data types, as the general assumption is that our data will be time-ordered. Most/all CDF tools will automatically convert the epoch to user-friendly dates and times.\n",
    "- HDF5 is the most widely used self-describing format\n",
    "  - The description above is basically that of HDF5 `groups` and `datasets`.\n",
    "  - SpacePy's internal data representation is modeled after the HDF5 data model\n",
    "  - There's an older version of HDF (HDF4) which isn't compatible with HDF5, but you're unlikely to encounter it in the wild.\n",
    "- NetCDF4 is built on top of HDF5\n",
    "  - NetCDF3 can be read using the NetCDF4 library, but *_not_* directly with the HDF5 library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Python environment\n",
    "\n",
    "Let's get started by importing all of the modules we'll need. If you've installed the prerequisite packages then you should be able to work with any of these data types.\n",
    "\n",
    "To stay organized, let's import modules in order:\n",
    "1. Python standard library\n",
    "2. Third-party: The _scientific stack_\n",
    "3. Third-party: Everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This unreleased version of SpacePy is not supported by the SpacePy team.\n",
      "Qin-Denton/OMNI2 data not found in current format. This module has limited functionality.\n",
      "Run spacepy.toolbox.update(QDomni=True) to download data\n"
     ]
    }
   ],
   "source": [
    "#standard library\n",
    "import os\n",
    "import glob\n",
    "from ftplib import FTP\n",
    "import urllib.request\n",
    "#scientific stack\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from matplotlib import pyplot as plt\n",
    "#everything else\n",
    "import spacepy.toolbox as tb\n",
    "from spacepy import pycdf\n",
    "import spacepy.datamodel as dm\n",
    "import spacepy.plot as splot\n",
    "\n",
    "#juypter/ipython magic command for inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "#And we'll set a variable for the directory with the data files,\n",
    "#in case you aren't using the github repo, or just want to point\n",
    "#to a different place.\n",
    "mydatapath = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting files\n",
    "\n",
    "If you have grabbed these data files in advance, great! If not, let's get them now...\n",
    "This is partly because we need files to demonstrate how to work with files, and partly as a basic reference for how to fetch files from the internet. It's fairly straightforward to start doing this programmatically in a workflow.\n",
    "\n",
    "First, we'll grab a NASA CDF file with THEMIS data from the NASA Space Physics Data Facility. There are sveral ways to do this, but we'll use a basic, generic FTP transfer. Since we already know eaxctly what the file is, and where it is, this method works just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the input/output file name\n",
    "fname = 'thd_l2_gmom_20120115_v01.cdf'\n",
    "localfname = os.path.join(mydatapath, fname)\n",
    "if not os.path.isfile(localfname):\n",
    "    #now open a connection to the SPDF FTP server and log in.\n",
    "    #It's an anonymous FTP server, so we don't need login credentials\n",
    "    ftp = FTP('spdf.gsfc.nasa.gov')\n",
    "    ftp.login()\n",
    "    #change directory to the location of the file we want\n",
    "    ftp.cwd('pub/data/themis/thd/l2/gmom/2012')\n",
    "    #now retrieve it and log out\n",
    "    with open(fname, 'wb') as fh:\n",
    "        ftp.retrbinary('RETR {0}'.format(fname), fh.write, 1024)\n",
    "    ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab an IDL save set. I've made a simple file that contains two variables for the pruposes of demonstrating this. Unless you downloaded or cloned the whole repository, then we'll have to grab it from the web. For this we'll need to use Python's basic web handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "guvifn = os.path.join(mydatapath, 'guvi_aurora_2003_197')\n",
    "guvi_url = 'http://guvitimed.jhuapl.edu/data/level3/guvi_aurora/data/IDLsave/2003/guvi_aurora_2003_197.sav'\n",
    "\n",
    "req = urllib.request.Request(guvi_url)\n",
    "#depending on where you are, web access might go through a proxy server\n",
    "#in that case you'd want to explicitly set your proxy by uncommenting the next two lines\n",
    "#proxy = 'proxy.example.edu:1405'\n",
    "#req.set_proxy(proxy, 'http')\n",
    "if not os.path.isfile(guvifn):\n",
    "    # Download the file from `url` and save it locally under `file_name`:\n",
    "    with urllib.request.urlopen(req) as response, open(guvifn, 'wb') as outfile:\n",
    "        gdata = response.read()\n",
    "        outfile.write(gdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next file is a NetCDF3 file with data from the AMPERE constellation. Since the AMPERE site requires downloaders to have an account, this file is in the GEM\\_2019 github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampfile = '20120109.1300.1200.600.north.grd.ncdf'\n",
    "ampfn = os.path.join(mydatapath, ampfile)\n",
    "amp_url = 'https://github.com/gemcommunity/gem_2019/blob/master/data/{0}'.format(ampfile)\n",
    "\n",
    "req = urllib.request.Request(amp_url)\n",
    "#do proxy stuff if required\n",
    "if not os.path.isfile(ampfn):\n",
    "    #retrieve from github\n",
    "    with urllib.request.urlopen(req) as response, open(guvifn, 'wb') as outfile:\n",
    "        adata = response.read()\n",
    "        outfile.write(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text\n",
    "\n",
    "We'll start with regular ASCII, or *_delimiter separated values_*. You've heard of CSV? That's _comma separated values_. If you use whitespace or tabs, for example, then you have DSV. Since this notebook should act as a reference, we'll do several methods. If you're in the room at GEM, we'll just use `numpy` to load the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file, rather awkwardly, has both ':' and '#' a to mark header lines.\n",
    "```\n",
    ":Data_list: Gp_part_5m.txt\n",
    ":Created: 2019 Jun 17 1816 UTC\n",
    "# Prepared by the U.S. Dept. of Commerce, NOAA, Space Weather Prediction Center\n",
    "...\n",
    "#                 Modified Seconds\n",
    "# UTC Date  Time   Julian  of the\n",
    "# YR MO DA  HHMM    Day     Day     P > 1     P > 5     P >10     P >30     P >50     P>100     E>0.8     E>2.0     E>4.0\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "2019 06 17  1615   58651  58500   3.71e+00  3.19e-01  2.78e-01  1.84e-01  1.28e-01  9.04e-02  1.67e+04  6.65e+01 -1.00e+05\n",
    "```\n",
    "First, we'll do this a native-Python way, and put our results into a structure that mimics the data model described at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Plain \"hand-rolled\" Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header (start and end only):\n",
      "\n",
      ":Data_list: Gp_part_5m.txt\n",
      ":Created: 2019 Jun 17 1816 UTC\n",
      "...\n",
      "# UTC Date  Time   Julian  of the\n",
      "# YR MO DA  HHMM    Day     Day     P > 1     P > 5     P >10     P >30     P >50     P>100     E>0.8     E>2.0     E>4.0\n",
      "#-------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('data/goes-particle-flux-primary.txt') as fh:\n",
    "    # this will read EVERYTHING from the file\n",
    "    #each row willbe a string\n",
    "    tempdata = fh.readlines()\n",
    "tempdata = [line.strip() for line in tempdata]\n",
    "#strip() #removes line breaks, trailing blanks, etc.\n",
    "\n",
    "gheader = [line for line in tempdata if line[0] in [':', '#']]\n",
    "print('Header (start and end only):\\n\\n{0}\\n{1}'.format(gheader[0], gheader[1]))\n",
    "print('...\\n{0}\\n{1}\\n{2}'.format(gheader[-3], gheader[-2], gheader[-1]))\n",
    "\n",
    "gbody = [line.split() for line in tempdata if line[0] not in [':', '#']] #breaks each line into parts, splitting on whitespace\n",
    "gbody = np.asarray(gbody)\n",
    "\n",
    "#now let's make a dictionary so we can access by variable name, then we'll put arrays inside it...\n",
    "goesdata = dict()\n",
    "goesdata['year'] = gbody[:, 0].astype(int)\n",
    "goesdata['month'] = gbody[:, 1].astype(int)\n",
    "goesdata['day'] = gbody[:, 2].astype(int)\n",
    "goesdata['seconds_of_day'] = gbody[:, 5]\n",
    "goesdata['flux_p'] = gbody[:, 6:12]\n",
    "goesdata['flux_e'] = gbody[:, 12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n",
      "|____day (numpy.ndarray (24,))\n",
      "|____flux_e (numpy.ndarray (24, 3))\n",
      "|____flux_p (numpy.ndarray (24, 6))\n",
      "|____month (numpy.ndarray (24,))\n",
      "|____seconds_of_day (numpy.ndarray (24,))\n",
      "|____year (numpy.ndarray (24,))\n"
     ]
    }
   ],
   "source": [
    "tb.dictree(goesdata, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using numpy's loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GOES data has dimensions (24, 15)\n",
      "Values in first row:\n",
      " [ 2.0190e+03  6.0000e+00  1.7000e+01  1.6150e+03  5.8651e+04  5.8500e+04\n",
      "  3.7100e+00  3.1900e-01  2.7800e-01  1.8400e-01  1.2800e-01  9.0400e-02\n",
      "  1.6700e+04  6.6500e+01 -1.0000e+05]\n"
     ]
    }
   ],
   "source": [
    "#We'll use numpy's loadtxt function to read the data and ignore the header.\n",
    "goesdata_np = np.loadtxt('data/goes-particle-flux-primary.txt', comments=['#',':'])\n",
    "\n",
    "#now inspect the shape of the data, so we know what array dimensions we are working with\n",
    "print('The GOES data has dimensions {0}'.format(goesdata.shape))\n",
    "\n",
    "#and we'll inspect the first line, which should be 15 elements long\n",
    "print('Values in first row:\\n {0}'.format(goesdata[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can either access the array directly whenever we want to use it, or copy the code from above to put it into a dictionary. Or, ...\n",
    "\n",
    "### Method 3: loadtxt, with a record array\n",
    "\n",
    "we can specify the data types in advance, and numpy will give us a \"record array\" that we can access by name. This time we'll just keep the columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (0, 1, 2, 5, 12, 13, 14) #time info and electron flux data\n",
    "names = ('year', 'month', 'day', 'seconds_of_day', 'flux_e1', 'flux_e2', 'flux_e3')\n",
    "datatypes = (np.int, np.int, np.int, np.float, np.float, np.float, np.float)\n",
    "goesdata_np = np.loadtxt('data/goes-particle-flux-primary.txt', comments=['#',':'],\n",
    "                         usecols=cols, dtype={'names': names, 'formats': datatypes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values in first row:\n",
      " 2019 6 17 58500.0 16700.0 66.5 -100000.0\n"
     ]
    }
   ],
   "source": [
    "print('Values in first row:\\n {0} {1} {2} {3} {4} {5} {6}'.format(*[goesdata_np[nn][0] for nn in names]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use SpacePy to convert a record array to a dictionary-like construction. We'll overwrite the old one, just for the convenience of having the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacepy.datamodel.SpaceData'>\n",
      "+\n",
      "|____day (spacepy.datamodel.dmarray (24,))\n",
      "|____flux_e1 (spacepy.datamodel.dmarray (24,))\n",
      "|____flux_e2 (spacepy.datamodel.dmarray (24,))\n",
      "|____flux_e3 (spacepy.datamodel.dmarray (24,))\n",
      "|____month (spacepy.datamodel.dmarray (24,))\n",
      "|____seconds_of_day (spacepy.datamodel.dmarray (24,))\n",
      "|____year (spacepy.datamodel.dmarray (24,))\n"
     ]
    }
   ],
   "source": [
    "goesdata = dm.fromRecArray(goesdata_np)\n",
    "\n",
    "print(type(goesdata))\n",
    "goesdata.tree(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the same as the _hand-rolled_ version above, but now the dictionary-like container carries metadata, as do the arrays. By default, these are empty, but we'll come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glabal metadata (should be empty here): {}\n",
      "Metadata on \"day\" (should be empty here): {}\n"
     ]
    }
   ],
   "source": [
    "print('Glabal metadata (should be empty here): {0}'.format(goesdata.attrs))\n",
    "print('Metadata on \"day\" (should be empty here): {0}'.format(goesdata['day'].attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy IDL save sets\n",
    "\n",
    "IDL has a long history in space physics, and the convenience of dumping the variables in an environment to a file has led to data being distributed in IDL's \"save set\" format. \n",
    "\n",
    "Thankfully, you don't need IDL to use an IDL save set any more! `scipy` is a core part of the scientific Python ecosystem, and it has had support for reading IDL save sets for a fairly long time.\n",
    "\n",
    "_NOTE_: Some IDL data types aren't supported, in my experience. Null pointers, for example. That means that occasionally you'll find an IDL saveset you just can't read with `scipy`. Unfortunately the way around that is to get access to a licensed copy, read the saveset in, then write it back out as a different file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seconds_of_day': array([1, 2, 3, 4, 5], dtype=int16), 'flux': array([45, 50, 35, 20, 55], dtype=int16)}\n",
      "\n",
      "+\n",
      "|____flux\n",
      "|____seconds_of_day\n"
     ]
    }
   ],
   "source": [
    "idldata = scio.readsav('data/test_idlsav.sav')\n",
    "print(idldata)\n",
    "print()\n",
    "tb.dictree(idldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `readsav` generates a dictionary containing arrays for each named variable.\n",
    "\n",
    "\n",
    "## Working with NetCDF3\n",
    "\n",
    "There are a lot of different binary formats used for data. Thankfully, the days of proprietary binary data requiring a minimally-documented code from the instrument team (that probably won't compile on your system) are just about over. Similarly, the days of needing a specific commercial software package just to read a file are pretty much over. The free self-describing file formats are more widely supported, and better in almost every way, than the proprietary formats these days.\n",
    "\n",
    "NetCDF is widely used in Earth and atmospheric sciences, and a lot of Earth-observing data uses it. NetCDF3 is a legacy version that has been superseded by NetCDF4, so we'll just see where the tools are and move on. Again, `scipy` provides the ability to work with NetCDF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the file gives us a file-like object: <scipy.io.netcdf.netcdf_file object at 0x0000017D8EF1E208>\n",
      "\n",
      "Let's look at what's in it... (just the first 7 variables)\n",
      "['nlon', 'nlat', 'start_yr', 'start_mo', 'start_dy', 'start_hr', 'start_mt']\n",
      "\n",
      "And now we can inspect the data we just read in.\n",
      "\n",
      "+\n",
      "|____nlon (numpy.ndarray (2,))\n",
      "\n",
      "ampdata_copy['nlon'] = [24 24]\n"
     ]
    }
   ],
   "source": [
    "with scio.netcdf.netcdf_file(ampfn) as ampdata:\n",
    "    print('Opening the file gives us a file-like object: {0}'.format(ampdata))\n",
    "    print(\"\\nLet's look at what's in it... (just the first 7 variables)\")\n",
    "    print([var for idx, var in enumerate(ampdata.variables) if idx<=6])\n",
    "    #to access the data we have to copy it from the file to a variable\n",
    "    ampdata_copy = dict()\n",
    "    ampdata_copy['nlon'] = ampdata.variables['nlon'][:].copy()\n",
    "\n",
    "print('\\nAnd now we can inspect the data we just read in.\\n')\n",
    "tb.dictree(ampdata_copy, verbose=True)\n",
    "print(\"\\nampdata_copy['nlon'] = {0}\".format(ampdata_copy['nlon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've extolled the virtues of having metadata, let's access the metadata on 'nlon'. The `with` block above closes the file on exit, so we're going to have to open it again...\n",
    "I'll just print it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Longitudinal array dimension'\n"
     ]
    }
   ],
   "source": [
    "with scio.netcdf.netcdf_file(ampfn) as ampdata:\n",
    "    print(ampdata.variables['nlon'].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, SpacePy's datamodel also provides a one-line read from NetCDF3 into SpacePy's data model.\n",
    "\n",
    "_NOTE_: The main reason you might not want to use these convenience methods is for very large files. If your file won't fit into memory, the convenience of the \"_suck all of the data into memory_\" approach will obviously fail. Then you'll have to fall back to the more manual methods above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ampdata_easy['nlon'] = [24 24]\n"
     ]
    }
   ],
   "source": [
    "ampdata_easy = dm.fromNC3(ampfn)\n",
    "\n",
    "print(\"\\nampdata_easy['nlon'] = {0}\".format(ampdata_easy['nlon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now all the metadata comes along for the ride, so we can inspect it by just looking at the `attrs` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('description', b'Longitudinal array dimension')])\n"
     ]
    }
   ],
   "source": [
    "print(ampdata_easy['nlon'].attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5, NetCDF4, and Matlab save files\n",
    "\n",
    "HDF5 is the current generation of the Heirarchical Data Format. It's been around since about 2002, and it's broadly used across the sciences. HDF5 has great parallel support and is widely adopted across high-performance comupting.\n",
    "\n",
    "So why are NetCDF4 and Matlab save files listed here? Well, NetCDF4 is built on top of HDF5. Since version 7 of Matlab, the default save format (the `.mat` saveset) has used HDF5 under the hood. So, unless the files are using either specific features not supported by Python interfaces to the HDF5 library, then reading NetCDF4 and `.mat` files is as easy as reading HDF5.\n",
    "\n",
    "The two major libraries that provide HDF5 support are `h5py` and `pytables`. `spacepy` provides convenience routines to read/write in one line through its `datamodel` module. As before, files that won't fit in memory shouldn't try to use the convenience routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA CDF\n",
    "\n",
    "And finally, NASA's Common Data Format (CDF). This really hasn't seen much use outside of heliophysics, so tools aimed at a broader community (like `scipy`) don't provide access to CDF.\n",
    "\n",
    "The Python tools that do are (in order of appearance):\n",
    "1. spacepy\n",
    "  - Originally released in 2009, this library has had full CDF support (read, write, etc.) since around 2010. It provides a robust interface to the NASA CDF library.\n",
    "    - Benefit: When the CDF library updates, as it does regularly, you just install the new one and SpacePy will use it. No waiting for the developers!\n",
    "    - Benefit: Provides full, robust, well-tested CDF library access. Read and write, supports backwards-compatible versions.\n",
    "    - Drawback: You have to install a C library (but NASA's instructions are pretty good).\n",
    "2. pysatCDF\n",
    "  - pysatCDF was designed to provide a lightweight, easy-to-install, CDF reader. It was primarily aimed at users of pysat (largely the CEDAR community). The \"easy-to-install\" part comes from the fact that the CDF library is bundled with it.\n",
    "    - Benefit: CDF is included, and `pysatCDF` will try to build it for you.\n",
    "    - Benefit: Syntax for use is modeled on `spacepy`, so the two are fairly interoperable.\n",
    "    - Drawback: If you need a new version of CDF you have to wait for `pysatCDF` to be updated, then reinstall that.\n",
    "    - Drawback: Only has read capability, no write capability.\n",
    "3. cdflib\n",
    "  - Originally (I believe) written for MAVEN, this is a pure Python version of the CDF library. It's only been around for a couple of years.\n",
    "    - Benefit: It's just Python. No need to worry about compiling C code, or having someone else compile it. It's just Python.\n",
    "    - Drawback: Any changes to how CDF works under-the-hood will need to be implemented in `cdflib` after CDF updates the C library.\n",
    "    - Drawback: Can only write v3 CDFs\n",
    "\n",
    "For the sake of interoperability I'll focus on using SpacePy. Reading using `pysatCDF` should work just about the same way as using the `spacepy.pycdf` module. `cdflib` has different syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
